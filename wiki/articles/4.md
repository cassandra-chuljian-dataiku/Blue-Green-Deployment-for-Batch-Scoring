# Flow Zones Deep-Dive

## [_Training](flow_zone:default)

The Training Flow zone is used to train new model versions on more recent data.
**Input:**
- Labelled Historical data to use for training [Histrical Training Data](dataset:dataiku_input_prepared)
  - This dataset requires a unique ID
  
**Output**
- **It will generate two datasets**
- Daily records to score [records_to_score](dataset:records_to_score) &
- A labelled evaluation data to feed to an MES for drift analysis [new_data](dataset:new_data)

- **Saved Models**
  - [BLUE Saved Model](saved_model:tXLDWR2b) &
  - [GREEN_Saved Model](saved_model:naPZZE1t)

The Blue & Green models will alternate representing the **Champion** and **Challenger** Models.
- The **Champion Model** is the model validated and in production
- The **Challenger Model** is the newly trained  model that will be progressively deployed to replace the Champion model.

## [_Split Strategy](flow_zone:zxmghd6)

The Split Strategy Flow Zone is used to define the splitting mechanism between Champion & Challenger models.
**Input:**
- Daily [records_to_score](dataset:records_to_score) (with a unique identifier column)

**Output:**
 - A [Split_strategy](dataset:Split_strategy) dataset containing  _splitting column_  for each stage of the deployment process tagging each record with *CHALLENGER* or *CHAMPION*.
  - In our case, we have 3 stages : 10/90 split, 50/50 split & fully deployed.

 _Note: the current split mechanism is using a random split but could be changed to any business logic_ 

## [_Model Evaluation](flow_zone:XPCSR62)

The Model Evaluation Flow Zone is used to **evaluate the Challenger and the Champion** model in order to compare them on the same evaluation dataset [new_data](dataset:new_data).
Note : When the Challenger is fully deployed as the Champion, this evaluation section will be used to compute both the Champion model data and performance drift.

**Input:**
- A labelled evaluation data to feed to an MES for drift analysis [new_data](dataset:new_data)
- [BLUE Saved Model](saved_model:tXLDWR2b) &
- [GREEN_Saved Model](saved_model:naPZZE1t)

**Output**
- [_BLUE_PRICE_RANK_EVAL](model_evaluation_store:ARFVe8tZ)
- [_GREEN_PRICE_RANK_EVAL](model_evaluation_store:93Og89Gv)

The Model Evaluation Stores are used to configure the evaluation settings and **set the checks** we want to be run at every run of the evaluation of the deployment. Ex : 
- Set the features to consider for Data Drift
- Check Data Drift/ Data Drift p-value metrics
- Check model performance metrics & Model performance drift.
- etc.


## [_Scoring](flow_zone:0PbNYNt)

The Scoring Flow Zone is used to score the [daily data to score](dataset:records_to_score) with the Champion and Challenger models. The predictions from the Champion and Challenger will then be selected based on the model deployment state and the [splitting strategy](dataset:Split_strategy).

**Input**
- Daily records to price [records_to_score](dataset:records_to_score)
- [BLUE Saved Model](saved_model:tXLDWR2b) &
- [GREEN_Saved Model](saved_model:naPZZE1t)

**Output**
- [Final Predictions](dataset:predicted_prices)