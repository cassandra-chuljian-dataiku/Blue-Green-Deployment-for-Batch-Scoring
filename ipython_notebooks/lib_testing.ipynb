{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv--compute-cpu-small",
      "display_name": "Python in compute-cpu-small (builtin env)",
      "language": "python"
    },
    "hide_input": false,
    "language_info": {
      "name": "python",
      "version": "3.9.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "creator": "pierre.petrella",
    "modifiedBy": "pierre.petrella",
    "tags": [],
    "createdOn": 1700648140347,
    "customFields": {}
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku\nfrom dataiku import pandasutils as pdu\nimport pandas as pd"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from blue_green_deployment import (get_project_var, set_project_var, get_logger, get_metric_from_mes)\nlogger \u003d get_logger()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "client \u003d dataiku.api_client()\nproject \u003d client.get_default_project()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "get_metric_from_mes(\"ARFVe8tZ\", \"DATA_DRIFT\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "get_metric_from_mes(\"ARFVe8tZ\", \"AUC\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#model_eval \u003d mes.list_model_evaluations()[0]\n#model_eval.get_metrics()[\"metrics\"]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mes_id \u003d \"ARFVe8tZ\"\nmetric_type \u003d \"DATA_DRIFT\"\nclient \u003d dataiku.api_client()\nproject \u003d client.get_default_project()\nmes \u003d project.get_model_evaluation_store(mes_id)\nlatest_eval \u003d mes.get_latest_model_evaluation()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get checks results.\nfor metric in latest_eval.get_metrics()[\"metrics\"]:\n    #print(metric[\"metric\"][\"metricType\"])\n    if metric[\"metric\"][\"id\"] \u003d\u003d \"check:CHECK:Value in range of AUC\":\n        print (metric[\"lastValues\"][0][\"value\"])\n    \n    if metric[\"metric\"][\"id\"] \u003d\u003d \"check:CHECK:Value in range of Data Drift p-value\":\n        print (metric[\"lastValues\"][0][\"value\"])\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Search for checks.\nfor metric in latest_eval.get_metrics()[\"metrics\"]:\n    #print(metric[\"metric\"][\"metricType\"])\n    #print(metric[\"meta\"][\"name\"])\n    if metric[\"metric\"][\"id\"] \u003d\u003d \"check:CHECK:Data dr\":\n        print (metric)\n        "
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "drift_results \u003d latest_eval.compute_drift()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "eval_full_info \u003d latest_eval.get_full_info()\neval_full_info.get_raw()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(drift_results.get_raw().keys())\ndata_drift_results \u003d drift_results.get_raw()[\u0027driftModelResult\u0027]\nprediction_drift_results \u003d drift_results.get_raw()[\u0027predictionDriftResult\u0027]\ndata_drift_results[\"driftModelAccuracy\"][\"value\"]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for metric in latest_eval.get_metrics()[\"metrics\"]:\n    if metric[\"metric\"][\"metricType\"] \u003d\u003d metric_type:\n        print(metric)       "
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        ""
      ],
      "outputs": []
    }
  ]
}